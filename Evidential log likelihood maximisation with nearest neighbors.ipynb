{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22de4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import faiss, sklearn, pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89b4f0",
   "metadata": {},
   "source": [
    "First we import the functions required to (a) predict the mass $\\hat{m}$ an uncertain point (b) calculate the evidential log likelihood and (c) calculate the gradient of the evidential loglikelihood with respect to the decays parameters.\n",
    "\n",
    "The Frame of Discernment is composed of $\\{F,NF, \\Theta\\}$ (Fraud, non-fraud and \"uncertain\" between the two)\n",
    "\n",
    "Each mass in the neighborhood is discounted according to: $m'_j(F) = m_j(F) \\alpha e^{- \\gamma_1 d^2_{ij}}$ and $m'_j(NF) = m_j(NF) \\alpha e^{- \\gamma_2 d^2_{ij}}$ and then pooled according to $$\\hat{m} = \\oplus_{x_i \\in \\mathcal{N}_k(x)} m'_i$$ where $\\oplus$ is the dempster rule of combination and $m_j$ the respective masses of observations $x_j$ in the neighborhood $\\mathcal{N}_k$ of a new observation $x$.\n",
    "\n",
    "Via the gradient we try to estimate the \"best\" values for the discount parameters $(\\alpha, \\gamma_1,\\gamma_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83d8d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functions\n",
    "def Gradient(decays, I, D, pl_train, pl_test):\n",
    "    # Algorithm 1. Gradient of the loglikelihood with respect to decays parameters, cf Denoeux 2019.\n",
    "    dists = D ** 2\n",
    "    k = I.shape[1]\n",
    "    alpha = 1 / (1 + np.exp(decays[0]))\n",
    "    gamma = decays[1:] ** 2\n",
    "    beta = alpha * (np.exp(-gamma.reshape(1, 1, 2) * dists.reshape(dists.shape[0], dists.shape[1], 1)))\n",
    "    discounted_pl = 1 - beta + (beta * pl_train[I])\n",
    "    hat_pl = discounted_pl[:, 0, :]\n",
    "    for n in range(1, k - 1):\n",
    "        hat_pl = hat_pl * discounted_pl[:, n, :]\n",
    "    norm_hat_pl = hat_pl / (hat_pl.sum(axis=1).reshape(hat_pl.shape[0], 1))\n",
    "\n",
    "    # gradient calculation cf Algorithm 3 Denoeux et al. 2019\n",
    "    a4 = pl_test / np.expand_dims((norm_hat_pl * pl_test).sum(axis=1), axis=1)  # A.4 (i,q)\n",
    "    a8 = (np.expand_dims(-hat_pl, axis=1) * (1 - pl_train[I])) / (\n",
    "            1 - (beta * (1 - pl_train[I])))  # A.8 (i,K,q) !!! not ok check denominator\n",
    "    a9 = - beta * np.expand_dims(dists, axis=2)  # A.9 (i,K,q)\n",
    "    a13 = beta / alpha  # A.13 (i,K,q)\n",
    "    a7 = (a8 * a9).sum(axis=1)  # A.7 (i,q)\n",
    "    a12 = (a8 * a13).sum(axis=1)  # A.12 (i,q)\n",
    "    a6_1 = (1 - norm_hat_pl) / (hat_pl.sum(axis=1).reshape(hat_pl.shape[0], 1))  # (i,q=0,k=0) & (i,q=1,k=1)\n",
    "    a6_2 = -hat_pl / (hat_pl.sum(axis=1).reshape(hat_pl.shape[0], 1) ** 2)  # (i,q=0,k=1) & (i,q=1,k=0)\n",
    "    a6 = np.stack((np.stack((a6_1[:, 0], a6_2[:, 0]), axis=1),\n",
    "                   np.stack((a6_2[:, 1], a6_1[:, 1]), axis=1)),\n",
    "                  axis=2)  # (i,k,q)\n",
    "    a5 = a6 * np.expand_dims(a7, axis=2)  # (i,k,q)\n",
    "    a3 = 2 * np.expand_dims(decays[1:], axis=0) * (np.expand_dims(a4, axis=1) * a5).sum(\n",
    "        axis=2)  # (i,k) # why is second term symetric ?\n",
    "    a11 = (a6 * np.expand_dims(a12, axis=2)).sum(axis=1)\n",
    "    a10 = alpha * (1 - alpha) * (a4 * a11).sum(axis=1)\n",
    "    a1 = a3.sum(axis=0)\n",
    "    a2 = a10.sum(axis=0)\n",
    "    return np.concatenate((np.array([a2]),a1))\n",
    "def eloglik(decays, I, D , pl_train, pl_test):\n",
    "    # Algorithm 1. Calculation of the evidential likelihood, cf Denoeux 2019.\n",
    "    dists = D ** 2 # distance are squared\n",
    "    k = I.shape[1]\n",
    "    alpha = 1 / (1 + np.exp(decays[0])) # variable transformation for optimisation cf Denoeux 2019\n",
    "    gamma = decays[1:] ** 2 # variable transformation for optimisation cf Denoeux 2019\n",
    "    beta = alpha * (np.exp(-gamma.reshape(1, 1, 2) * dists.reshape(dists.shape[0], dists.shape[1], 1)))\n",
    "    discounted_pl = 1 - beta + (beta * pl_train[I]) # neighbors' plausibilities discounted\n",
    "    hat_pl = discounted_pl[:, 0, :] # first neighbor's plausibility\n",
    "    for n in range(1, k - 1): # aggregate over neighbor \n",
    "        hat_pl = hat_pl * discounted_pl[:, n, :]\n",
    "    norm_hat_pl = hat_pl / (hat_pl.sum(axis=1).reshape(hat_pl.shape[0], 1))\n",
    "    # auc if needed\n",
    "    # fpr, tpr, thresholds = sklearn.metrics.roc_curve(np.round(pl_test)[:,0], norm_hat_pl[:,0])\n",
    "    # print(sklearn.metrics.auc(fpr, tpr))\n",
    "    return -np.sum(np.log((norm_hat_pl * pl_test).sum(axis=1)))\n",
    "def predict_eknn(decays, I, D, my):\n",
    "    # Aggregation of masses over a (weighted, or discounted) neighborhood\n",
    "    nb = I\n",
    "    dists = D ** 2 #preds[0]\n",
    "    k = nb.shape[1]\n",
    "    alpha = 1 / (1 + np.exp(decays[0]))\n",
    "    gamma = decays[1:] ** 2\n",
    "    m0 = my[nb][:,:,0] * alpha * (np.exp(-gamma[0] * dists)) #* (np.exp(-time_decay * times))\n",
    "    m1 = my[nb][:,:,1] * alpha * (np.exp(-gamma[1] * dists)) #* (np.exp(-time_decay * times))\n",
    "    m10 = 1 - m0 - m1\n",
    "    m0t = m0[:,0]\n",
    "    m1t = m1[:,0]\n",
    "    m10t = m10[:,0]\n",
    "    for n in range(k-1):\n",
    "        denominators = 1 - (m0t * m1[:,n+1] + m0[:,n+1] * m1t) # denominators\n",
    "        m0t = (m0t * m10[:,n+1] + m0[:,n+1] * m10t + m0t * m0[:,n+1])/denominators\n",
    "        m1t = (m1t * m10[:,n+1] + m1[:,n+1] * m10t + m1t * m1[:,n+1])/denominators\n",
    "        m10t = 1 - m0t - m1t\n",
    "    return np.vstack((m0t,m1t,m10t)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab90331",
   "metadata": {},
   "source": [
    "We load the iris dataset and subset it to focus on two classes only.\n",
    "We also artficially introduce uncertainty on the labels to mimic imperfect labels (e.g. each flower has been observed with a certain degree of reliability).\n",
    "\n",
    "$m(\\{\\Theta\\})$ is the vacuous function and represents total uncertainty\n",
    "$m(\\{\\theta_0\\})$ and\n",
    "$m(\\{\\theta_1\\})$ are the masses of respectively class $\\theta_0$ (e.g., Fraud) and class $\\theta_1$ (e.g., Not Fraud).\n",
    "\n",
    "Each point in the training set $\\mathcal{T} = \\{ (x_i,m_i), i = 1,...,n\\}$ is assigned a mass $m_i(\\{\\theta_0\\})$, $m_i(\\{\\theta_1\\})$, $m_i(\\{\\Theta\\})$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60dab461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   $m(\\theta_0)$  $m(\\theta_1)$  $m(\\Theta)$\n",
      "0       0.791721       0.000000     0.208279\n",
      "1       0.894184       0.000000     0.105816\n",
      "2       0.871428       0.000000     0.128572\n",
      "3       0.825931       0.000000     0.174069\n",
      "4       0.000000       0.923634     0.076366\n",
      "5       0.000000       0.913915     0.086085\n",
      "6       0.000000       0.894069     0.105931\n",
      "7       0.000000       0.878683     0.121317\n",
      "8       0.000000       0.759366     0.240634\n",
      "9       0.000000       0.930676     0.069324\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris().data[1:100, [0, 2]], load_iris().target[1:100]\n",
    "# represents m(\\theta_0), m(\\theta_1) and m(\\Theta)\n",
    "my = np.zeros(shape=(y.size,3))\n",
    "my[:,0] = np.where(y == 0,1,0)\n",
    "my[:,1] = np.where(y == 1,1,0)\n",
    "my = (my.T * np.random.beta(5,1,my.shape[0])).T #add some uncertainty on the labels at random here, just to show how it would work with uncertain labels.\n",
    "my[:,2] = 1 - my[:,1] - my[:,0]\n",
    "pl = my[:, :-1] + my[:, -1].reshape(my.shape[0], 1)\n",
    "\n",
    "print(pd.DataFrame(my[45:55,:], columns=[\"$m(\\theta_0)$\", \"$m(\\theta_1)$\", \"$m(\\Theta)$\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef6cca",
   "metadata": {},
   "source": [
    "We can visualise our dataset, the color gradients are just there to represent the uncertainty of each point. \n",
    "We can clearly identify two groups nevertheless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a140a8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6a78f30e5142268f7ffa71bdf8199f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:,0], X[:,1],c=(my[:,0]+my[:,2]/2), edgecolor='k', s=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b87124f",
   "metadata": {},
   "source": [
    "Next we divide our dataset in a train and test sets, we calculate the neighbors of each observation in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4cab6b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, my_train, my_test, pl_train, pl_test = train_test_split(X,my,pl, test_size=0.2)\n",
    "index = faiss.IndexFlatL2(X_train.shape[1]) # In this case we use euclidean distance.\n",
    "index.add(np.ascontiguousarray(X_train, dtype=np.float32))  # should convert once to arrays np.float out of loop.\n",
    "D, I = index.search(np.ascontiguousarray(X_test, dtype=np.float32), k=5) # here k=5 neighbors, can be changed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce767272",
   "metadata": {},
   "source": [
    "Here we use Broyden–Fletcher–Goldfarb–Shanno method, a numerical method for numerical optimisation. \n",
    "Note that the gradient is used at this stage and we learn the parameters $(\\alpha, \\gamma_0, \\gamma_1)$. In this example we have very little information in the test set, so it might be unstable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85038696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.005858\n",
      "         Iterations: 12\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 65\n"
     ]
    }
   ],
   "source": [
    "res = minimize(eloglik, np.array((0, 1, 1)), method='BFGS', jac=Gradient,\n",
    "               options={'disp': True}, args=(I, D, pl_train, pl_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a7adfce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.47131808,  1.68339459,  1.12055593])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883e113",
   "metadata": {},
   "source": [
    "Finally we can visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05d7a507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b7331d782f4ee48d7ceb01064064df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x1c439ef4288>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_eknn_forgrid(X_test = [6, 4.5], index=index, resx = res.x, my = my_train):\n",
    "    D, I = index.search(np.ascontiguousarray(X_test, dtype=np.float32).reshape(1,2), k=5)\n",
    "    result = predict_eknn(resx, I=I, D=D, my=my)\n",
    "    return(result)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "h = .05\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "grid_to_assess = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "Z = np.apply_along_axis(predict_eknn_forgrid, 1, grid_to_assess)\n",
    "\n",
    "m1 = Z[:,0,0].ravel().reshape(xx.shape)\n",
    "m2 = Z[:,0,1].ravel().reshape(xx.shape)\n",
    "m3 = Z[:,0,2].ravel().reshape(xx.shape)\n",
    "\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, subplot_kw={\"projection\": \"3d\"})\n",
    "ax1.set_title(r'$m(\\theta_0)$')\n",
    "ax1.plot_surface(xx, yy, m1,cmap=cm.coolwarm,linewidth=0, antialiased=False,alpha=.5, label = 'mass')\n",
    "ax1.scatter(X[:,0], X[:,1],c=(my[:,0]+my[:,2]/2), label='training points')\n",
    "ax2.set_title(r'$m(\\theta_1)$')\n",
    "ax2.plot_surface(xx, yy, m2,cmap=cm.coolwarm,linewidth=0, antialiased=False,alpha=.5, label = 'mass')\n",
    "ax2.scatter(X[:,0], X[:,1],c=(my[:,0]+my[:,2]/2), label='training points')\n",
    "ax3.set_title(r'$m(\\Theta)$ Uncertainy')\n",
    "ax3.plot_surface(xx, yy, m3,cmap=cm.coolwarm,linewidth=0, antialiased=False,alpha=.5, label = 'mass')\n",
    "ax3.scatter(X[:,0], X[:,1],c=(my[:,0]+my[:,2]/2), label='training points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586c351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
