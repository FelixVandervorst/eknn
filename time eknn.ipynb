{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47c0b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import multivariate_normal, normal\n",
    "import faiss, sklearn\n",
    "from scipy.optimize import minimize\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "850f1a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two normal distributions (two classes) but one runs in a circle over time\n",
    "lenght_cycle = 100\n",
    "theta = np.linspace(0, 2*np.pi, lenght_cycle)\n",
    "radius = 5\n",
    "a = radius*np.cos(theta)\n",
    "b = radius*np.sin(theta)\n",
    "X=np.zeros((lenght_cycle*2,2))\n",
    "my=np.zeros((lenght_cycle*2,3))\n",
    "t=np.zeros((lenght_cycle*2))\n",
    "for i in range(lenght_cycle):\n",
    "    mean = [radius*np.cos(theta[i]), radius*np.sin(theta[i])]\n",
    "    cov = [[1, 0], [0, 1]]\n",
    "    X[2 * i+1, :] = multivariate_normal(mean, cov, size=1)\n",
    "    X[(2 * i), :] = multivariate_normal([0, 0], cov, size=1)\n",
    "    my[2 * i+1, 0] = 1 - min(0.5, normal(0, 0.3, size=1)[0] ** 2) # a little bit of uncertainty\n",
    "    my[(2 * i), 1] = 1 - min(0.5, normal(0, 0.3, size=1)[0] ** 2) # a little bit of uncertainty\n",
    "    t[2 * i+1] = i\n",
    "    t[(2 * i)] = i\n",
    "my[:,2] = 1 - my[:,0] - my[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5606d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "n = X.shape[0]\n",
    "D = np.zeros((n,k))\n",
    "I = np.zeros((n,k))\n",
    "index = faiss.IndexFlatL2(X.shape[1])\n",
    "index.add(np.ascontiguousarray(X[:k, :],\n",
    "                               dtype=np.float32))  # should convert once to arrays np.float out of loop.\n",
    "for i in range(k+1, n):\n",
    "    D[i, :], I[i, :] = index.search(np.ascontiguousarray(X[i, :].reshape(1, 2), dtype=np.float32), k=5)\n",
    "    index.add(np.ascontiguousarray(X[i,:].reshape(1,2), dtype=np.float32))  # should convert once to arrays np.float out of loop.\n",
    "\n",
    "I = I.astype(int)\n",
    "T = np.expand_dims(t, axis=1) - t[I] # relative time difference of neighbor j to observation i\n",
    "\n",
    "pl = my[:,:2] + np.expand_dims(my[:,2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "211dcddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient(decays, I, D, pl, T):\n",
    "    dists = D ** 2\n",
    "    times = T ** 2\n",
    "    k = I.shape[1]\n",
    "    alpha = 1 / (1 + np.exp(decays[0]))\n",
    "    gamma = decays[1:3] ** 2\n",
    "    tau = decays[3:5] ** 2\n",
    "    beta = alpha * (np.exp(-gamma.reshape(1, 1, 2) * dists.reshape(dists.shape[0], dists.shape[1], 1))) * np.exp(-tau.reshape(1, 1, 2) * times.reshape(dists.shape[0], dists.shape[1], 1))\n",
    "    discounted_pl = 1 - beta + (beta * pl[I])\n",
    "    hat_pl = discounted_pl[:, 0, :]\n",
    "    for n in range(1, k - 1):\n",
    "        hat_pl = hat_pl * discounted_pl[:, n, :]\n",
    "    norm_hat_pl = hat_pl / (hat_pl.sum(axis=1).reshape(hat_pl.shape[0], 1))\n",
    "    # gradient calculation cf Algorithm 3 Denoeux et al. 2019\n",
    "    a4 = pl / np.expand_dims((norm_hat_pl * pl).sum(axis=1), axis=1)  # A.4 (i,q)\n",
    "    a8 = (np.expand_dims(-hat_pl, axis=1) * (1 - pl[I])) / (\n",
    "            1 - (beta * (1 - pl[I])))  # A.8 (i,K,q) !!! not ok check denominator\n",
    "    a9 = - beta * np.expand_dims(dists, axis=2)  # A.9 (i,K,q)\n",
    "    a9t = - beta * np.expand_dims(times, axis=2)\n",
    "    a13 = beta / alpha  # A.13 (i,K,q)\n",
    "    a7 = (a8 * a9).sum(axis=1)  # A.7 (i,q)\n",
    "    a7t = (a8 * a9t).sum(axis=1)\n",
    "    a12 = (a8 * a13).sum(axis=1)  # A.12 (i,q)\n",
    "    a6_1 = (1 - norm_hat_pl) / (hat_pl.sum(axis=1).reshape(hat_pl.shape[0], 1))  # (i,q=0,k=0) & (i,q=1,k=1)\n",
    "    a6_2 = -hat_pl / (hat_pl.sum(axis=1).reshape(hat_pl.shape[0], 1) ** 2)  # (i,q=0,k=1) & (i,q=1,k=0)\n",
    "    a6 = np.stack((np.stack((a6_1[:, 0], a6_2[:, 0]), axis=1),\n",
    "                   np.stack((a6_2[:, 1], a6_1[:, 1]), axis=1)),\n",
    "                  axis=2)  # (i,k,q)\n",
    "    a5 = a6 * np.expand_dims(a7, axis=2)  # (i,k,q)\n",
    "    a5t = a6 * np.expand_dims(a7t, axis=2)\n",
    "    a3 = 2 * np.expand_dims(decays[1:3], axis=0) * (np.expand_dims(a4, axis=1) * a5).sum(\n",
    "        axis=2)  # (i,k)\n",
    "    a3t = 2 * np.expand_dims(decays[3:5], axis=0) * (np.expand_dims(a4, axis=1) * a5t).sum(\n",
    "        axis=2)  # (i,k)\n",
    "    a11 = (a6 * np.expand_dims(a12, axis=2)).sum(axis=1)\n",
    "    a10 = -alpha * (1 - alpha) * (a4 * a11).sum(axis=1) # NB: the '-' is omitted in the original paper (typo)\n",
    "    a1 = a3.sum(axis=0)\n",
    "    a1t = a3t.sum(axis=0)\n",
    "    a2 = a10.sum(axis=0)\n",
    "    return np.concatenate((np.array([a2]),a1, a1t))\n",
    "def eloglik(decays, I, D , pl, T):\n",
    "    dists = D ** 2\n",
    "    times = T ** 2 # if missing just set to 0 and we have back the original algo\n",
    "    k = I.shape[1]\n",
    "    alpha = 1 / (1 + np.exp(decays[0]))\n",
    "    gamma = decays[1:3] ** 2\n",
    "    tau = decays[3:5] ** 2\n",
    "    beta = alpha * (np.exp(-gamma.reshape(1, 1, 2) * dists.reshape(dists.shape[0], dists.shape[1], 1))) * np.exp(\n",
    "        -tau.reshape(1, 1, 2) * times.reshape(dists.shape[0], dists.shape[1], 1))\n",
    "    discounted_pl = 1 - beta + (beta * pl[I])\n",
    "    hat_pl = discounted_pl[:, 0, :]\n",
    "    for n in range(1, k - 1):\n",
    "        hat_pl = hat_pl * discounted_pl[:, n, :]\n",
    "    norm_hat_pl = hat_pl / (hat_pl.sum(axis=1).reshape(hat_pl.shape[0], 1))\n",
    "    # auc\n",
    "    #fpr, tpr, thresholds = sklearn.metrics.roc_curve(np.round(pl)[:,0], norm_hat_pl[:,0])\n",
    "    #print(sklearn.metrics.auc(fpr, tpr))\n",
    "    #print(np.sum(np.log((norm_hat_pl * pl).sum(axis=1))))\n",
    "    return -np.sum(np.log((norm_hat_pl * pl).sum(axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4cdf1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 155.767225\n",
      "         Iterations: 1\n",
      "         Function evaluations: 52\n",
      "         Gradient evaluations: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.27047992, -0.87310846,  0.10044858,  0.        ,  0.99942603])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = minimize(eloglik, np.array((0, 0.1, 0.1, 0,  1)), method='BFGS', jac=Gradient,\n",
    "               options={'disp': True}, args=(I, D, pl, T))\n",
    "res.x #to check and interpret results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be27909c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3957d2c8d0ee4b5ea604cb1d4f50c4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Evidential Knn for $m(\\\\mathscr{E} = , -0.0,\\\\eta_0 = -0.9, \\\\eta_1 = 0.1,\\\\tau_0 = 0.0,\\\\tau_1 = 1.0)$ at time 100 (end of animation)')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def predict_eknn_new(X_test = [4, 4], index=index, resx = res.x, my = my, t = t):\n",
    "    D, I = index.search(np.ascontiguousarray(X_test, dtype=np.float32).reshape(1,2), k=5) # the full history is there if we do it like this\n",
    "    T = 100 - t[I] # relative time difference of neighbor j to observation i\n",
    "    result = predict_eknn(resx, I=I, D=D, my=my, T=T)\n",
    "    return(result)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "h = .05\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "grid_to_assess = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "Z = np.apply_along_axis(predict_eknn_new, 1, grid_to_assess)\n",
    "\n",
    "m1 = Z[:,0, 0].ravel().reshape(xx.shape)\n",
    "m2 = Z[:,0, 1].ravel().reshape(xx.shape)\n",
    "m3 = Z[:,0, 2].ravel().reshape(xx.shape)\n",
    "\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, subplot_kw={\"projection\": \"3d\"})\n",
    "ax1.set_title(r'$m(\\theta_0)$')\n",
    "ax1.plot_surface(xx, yy, m1,cmap=cm.coolwarm,linewidth=0, antialiased=False,alpha=.5, label = 'mass')\n",
    "ax1.scatter(X[:,0], X[:,1],c=(my[:,0]+my[:,2]/2), label='training points')\n",
    "ax2.set_title(r'$m(\\theta_1)$')\n",
    "ax2.plot_surface(xx, yy, m2,cmap=cm.coolwarm,linewidth=0, antialiased=False,alpha=.5, label = 'mass')\n",
    "ax2.scatter(X[:,0], X[:,1],c=(my[:,0]+my[:,2]/2), label='training points')\n",
    "ax3.set_title(r'$m(\\Theta)$ Uncertainy')\n",
    "ax3.plot_surface(xx, yy, m3,cmap=cm.coolwarm,linewidth=0, antialiased=False,alpha=.5, label = 'mass')\n",
    "ax3.scatter(X[:,0], X[:,1],c=(my[:,0]+my[:,2]/2), label='training points')\n",
    "fig.suptitle(r'Evidential Knn for $m(\\mathscr{E} = , ' + str(round(res.x[0])) +\n",
    "             r',\\eta_0 = ' +str(round(res.x[1],  1)) +\n",
    "             r', \\eta_1 = ' +str(round(res.x[2], 1)) +\n",
    "             r',\\tau_0 = ' + str(round(res.x[3], 1)) +\n",
    "             r',\\tau_1 = ' + str(round(res.x[4], 1)) +\n",
    "             r')$ at time 100 (end of animation)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73a2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
